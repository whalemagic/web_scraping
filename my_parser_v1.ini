import requests
import re #регулярки
from bs4 import BeautifulSoup
from tqdm import tqdm  #для прогресс-бара
import time #sleep 1s
import pandas as pd
def author_name_from_product_name(product_name):
    match = re.search(r'by\s+(.+)', product_name)
    return match.group(1) if match else None

def get_product_details(product_url, page):
    try:
        response = requests.get(product_url)
        
        if response.status_code != 200:
            print(f"Ошибка при получении страницы {product_url}")
            return None
        
        soup = BeautifulSoup(response.text, 'html.parser')
        
        product_div = soup.find('div', id='product_name')
        product_name = product_div.find('h1').text.strip() if product_div and product_div.find('h1') else 'Нет данных'
        
        description_div = soup.find('div', id='product_description')
        product_description = description_div.text.strip() if description_div else 'Нет данных'
        
        price_tag = soup.find('td', class_='ourprice')
        price = price_tag.text.strip() if price_tag else "Цена не найдена."
        
        product_name = ' '.join(product_name.split())
        product_description = ' '.join(product_description.split())
        author_name = author_name_from_product_name(product_name)

        return {
            'product_name': product_name,
            'author_name': author_name,
            'price': price,
            'product_url': product_url,
            'page': page,
            'product_description': product_description,
            #категории
            #теги
            #убрать из author_name все теги
        }
    
    except requests.exceptions.RequestException as e:
        print(f"Произошла ошибка при запросе: {e}")
        return None

def get_all_product_details(base_url, start_page, end_page):
    product_details_list = []
    
    with tqdm(total=end_page - start_page + 1, desc="Загрузка страниц", unit="стр") as pbar:
        for page in range(start_page, end_page + 1):
            product_url = f"{base_url}/{page}"
            product_details = get_product_details(product_url, page)
            if product_details:
                product_details_list.append(product_details)
            pbar.update(1)  # Обновляем прогресс-бар
            time.sleep(1.5)  # Пауза в 1 секунду перед следующим запросом
    return product_details_list

if __name__ == "__main__":
    base_url = "https://www.penguinmagic.com/p" #пингвины
    start_page = 6001
    end_page = 7000
    product_details_list = get_all_product_details(base_url, start_page, end_page)
    
pd.DataFrame(product_details_list).query('product_name!="Нет данных"').to_excel("penguins6k-7k.xlsx")